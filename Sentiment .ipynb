{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db5fb177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "473dfee4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imblearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1832/1658042718.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munder_sampling\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomUnderSampler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'imblearn'"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report , roc_curve, f1_score, accuracy_score, recall_score , roc_auc_score,make_scorer\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from scipy import stats\n",
    "from sklearn.metrics import confusion_matrix,mean_squared_error,precision_score,recall_score,f1_score\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import qalsadi.lemmatizer\n",
    "\n",
    "\n",
    "#Models & parameters Tuning\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier \n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d1938c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import sys\n",
    "import argparse\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69451480",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import StanfordPOSTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b84f9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.isri import ISRIStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a78aba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4419f177",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Final_DataEncoded.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ecfb1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.drop(inplace=True,columns=['Unnamed: 0'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55229626",
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\n",
    "\n",
    "english_punctuations = string.punctuation\n",
    "punctuations_list = arabic_punctuations + english_punctuations\n",
    "stop_words = list(set(stopwords.words('arabic')))\n",
    "print(stop_words)\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc8ed62",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.remove('لا')\n",
    "stop_words.remove('لكن')\n",
    "stop_words.remove('ولكن')\n",
    "stop_words.remove('واو')\n",
    "stop_words.remove('أطعم')\n",
    "stop_words.remove('أف')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5feb4f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be52098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_diacritics(text):\n",
    "    arabic_diacritics = re.compile(\"\"\" ّ    | # Tashdid\n",
    "                             َ    | # Fatha\n",
    "                             ً    | # Tanwin Fath\n",
    "                             ُ    | # Damma\n",
    "                             ٌ    | # Tanwin Damm\n",
    "                             ِ    | # Kasra\n",
    "                             ٍ    | # Tanwin Kasr\n",
    "                             ْ    | # Sukun\n",
    "                             ـ     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)\n",
    "    text = re.sub(arabic_diacritics, '', str(text))\n",
    "    return text\n",
    "\n",
    "def remove_emoji(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word for word in text if word not in string.punctuation])\n",
    "    text = remove_emoji(text)\n",
    "    text = remove_diacritics(text)\n",
    "    tokens = word_tokenize(text)\n",
    "    text = ' '.join([word for word in tokens if word not in stop_words])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e692d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.review_description= df.review_description.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d142f1a1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['cleaned_text'] = df.review_description.apply(clean_text)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cca568e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909402bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df.cleaned_text.all()\n",
    "lemmer = qalsadi.lemmatizer.Lemmatizer()\n",
    "# lemmatize a word\n",
    "lemmer.lemmatize_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7f32cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['final_text'] = df.cleaned_text.apply(lambda x:lemmer.lemmatize_text(x,return_pos=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8b5294",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0d840d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('LOL_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9111f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('LOL_dataset.csv')\n",
    "df.drop(inplace=True,columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447f9ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580ae204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_list_to_str(data):\n",
    "    data = str(data)\n",
    "    data = data.replace(\"'\",'')\n",
    "    data = data.replace(',','')\n",
    "    data = data.replace('[','')\n",
    "    data = data.replace(']','')\n",
    "\n",
    "    return data\n",
    "\n",
    "df['final_text'] = df.final_text.apply(convert_list_to_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963e3477",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc7ade5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorizer=TfidfVectorizer(max_features=2000, ngram_range=(1, 3))\n",
    "text_features=vectorizer.fit_transform(df[\"final_text\"])\n",
    "my_array=text_features.toarray()\n",
    "df2=pd.DataFrame(my_array,columns=vectorizer.get_feature_names())\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4189ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = df2\n",
    "Y = df.rating\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc3f5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test =  train_test_split(X,Y,random_state=42,test_size=0.20,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe106bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(name, trained_model , x_train, y_train, x_test, y_test):\n",
    "    tree_predict = trained_model.predict(x_test)\n",
    "    print(\"Testing accuracy   :\",metrics.accuracy_score(y_test, tree_predict)*100 , \"%\")\n",
    "    print(\"MSE [TEST]          :\",mean_squared_error(y_test, tree_predict))\n",
    "\n",
    "\n",
    "    tree_predict1 = trained_model.predict(x_train)\n",
    "    print(\"Training accuracy  :\",metrics.accuracy_score(y_train, tree_predict1)*100 ,\"%\")\n",
    "    print(\"MSE [TRAIN]         :\",mean_squared_error(y_train, tree_predict1))\n",
    "\n",
    "    print(\"precision : \",precision_score(y_test, tree_predict,average='micro'))\n",
    "    print(\"recall    : \",recall_score(y_test, tree_predict,average='micro'))\n",
    "    print(\"f1_score  : \",f1_score(y_test, tree_predict,average='micro'))\n",
    "\n",
    "\n",
    "    cf1 = confusion_matrix(y_test,tree_predict)\n",
    "    sns.heatmap(cf1,annot=True,fmt = '.0f')\n",
    "    plt.xlabel('prediction')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(name+ ' Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    print(classification_report(y_train,  trained_model.predict(x_train)))\n",
    "    print(classification_report(y_test,  trained_model.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c457dd8c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "trained_clf_random_forest = RandomForestClassifier().fit(x_train, y_train)\n",
    "get_accuracy('RandomForestClassifier',trained_clf_random_forest,x_train, y_train, x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568a5dc8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "trained_clf_multinomial_nb = MultinomialNB().fit(x_train, y_train)\n",
    "get_accuracy('MultinomialNB',trained_clf_multinomial_nb,x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2dde50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "trained_clf_svc = LinearSVC().fit(x_train, y_train)\n",
    "get_accuracy('LinearSVC',trained_clf_svc,x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7576ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "trained_clf_LogisticRegression = LogisticRegression().fit(x_train, y_train)\n",
    "get_accuracy('LogisticRegression',trained_clf_LogisticRegression,x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3112ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14d5427",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb54531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fb5b91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d0d9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "# using tokenizer to transform text messages into training and testing set\n",
    "X_train_seq = tokenizer.texts_to_sequences(x_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(x_test)\n",
    "X_train_seq_padded = pad_sequences(X_train_seq, maxlen=64)\n",
    "X_test_seq_padded = pad_sequences(X_test_seq, maxlen=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4de4d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.word_counts)\n",
    "#print(tokenizer.document_count)\n",
    "#print(tokenizer.word_index)\n",
    "#print(tokenizer.word_docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
